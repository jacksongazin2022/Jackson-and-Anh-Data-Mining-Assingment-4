{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20ad65b",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe8befa-30dd-4af5-995f-a8309cdfe639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import my_utils\n",
    "import gzip\n",
    "import shutil\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd8c97-6e72-47c6-bee6-2a807d532e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_gz_file = '../Output/GSE185948_count_RNA.rds.gz'\n",
    "output_rds_file = '../Output/data_for_r.rds'\n",
    "\n",
    "# Open the compressed file and extract it\n",
    "with gzip.open(input_gz_file, 'rb') as f_in, open(output_rds_file, 'wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f'{input_gz_file} has been successfully uncompressed to {output_rds_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77773596-d217-481d-90eb-cff1fe9563ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_gz_file = '../Output/GSE185948_metadata_RNA.csv.gz'\n",
    "output_csv_file = '../Output/uncompressed_metadata.csv'\n",
    "\n",
    "# Open the compressed file and extract it\n",
    "with gzip.open(input_gz_file, 'rb') as f_in, open(output_csv_file, 'wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f'{input_gz_file} has been successfully uncompressed to {output_csv_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd10524-abd6-4eda-8ea8-9d53cff8d84c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = '../Output'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d033d150-def7-422c-b556-cfadb89c76db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('../Output/uncompressed_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12762e-d950-4f0d-88ae-cd32ec8e43ca",
   "metadata": {},
   "source": [
    "R code below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea593dcf-f6ef-476e-97fa-5527db6fb58a",
   "metadata": {},
   "source": [
    "filename = file.choose()\n",
    "data = readRDS(filename)\n",
    "data\n",
    "sparse_matrix <- data\n",
    "\n",
    "num_columns <- ncol(sparse_matrix)\n",
    "# Get the nonzero elements and their indices\n",
    "nonzero_elements <- sparse_matrix@x\n",
    "row_indices <- sparse_matrix@i\n",
    "col_indices <- rep(1:num_columns, times = diff(sparse_matrix@p))  # Use sparse_matrix@j for column indices\n",
    "\n",
    "\n",
    "nonzero_data <- data.frame(row_indices, col_indices, nonzero_elements)\n",
    "\n",
    "col_names <- data.frame(sparse_matrix@Dimnames[2])\n",
    "nrow(col_names)\n",
    "row_names <- data.frame((sparse_matrix@Dimnames[1]))\n",
    "\n",
    "write.csv(row_names, 'row_names.csv')\n",
    "write.csv(col_names, 'col_names.csv')\n",
    "\n",
    "\n",
    "# Save the i, j information to a Parquet file\n",
    "write_parquet(nonzero_data, \"non_zero.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c87489-add6-4698-bc23-884af1dcaf8c",
   "metadata": {},
   "source": [
    "transpose_test## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9be1fb-4878-40e1-a1c3-86aaa9775916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d881cd-1a18-43f9-84d1-ff85e77ef9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade --no-deps memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32d363c1-327c-4470-b520-af645a113edf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning sparse_matrix, column_names, row_names, and row_indices\n"
     ]
    }
   ],
   "source": [
    "data_path = '../Output/non_zero.parquet'\n",
    "row_info_path = '../Output/row_names.csv'\n",
    "column_info_path = '../Output/col_names.csv'\n",
    "#sparse_matrix = coo_matrix((data['nonzero_elements'], (data['row_indices'], data['col_indices'])))\n",
    "sparse_matrix, column_names, row_names, row_indices= my_utils.load_data(data_path, row_info_path, column_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a117b95c-3949-4aa3-80bf-4fdadc792516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transpose_test = sparse_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3fbaadb8-d193-4c28-99bb-b29b9c7eaa55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_indices_test = np.arange(transpose_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05d2af9c-651f-4680-a19d-0a29c9a9084c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 102707, 102708, 102709])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb6927-abb7-4961-a2b4-3447d71d8234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_pipeline = Pipeline([\n",
    "    ('splitter', my_utils.SparseTrainTestSplit(test_size=0.2, random_state=42, row_indices = row_indices)),\n",
    "    # Add other steps in the pipeline as needed\n",
    "])\n",
    "\n",
    "# Fit and transform the pipeline\n",
    "sparse_train, sparse_test, train_indices, test_indices = split_pipeline.fit_transform(sparse_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f17eec-f7e8-4790-b8f1-bd978367b2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfeb12c-ab56-424c-a09a-62dd30fc2494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_and_pca_pipeline = Pipeline([\n",
    "    ('cleaner', my_utils.DataClean()),  # CleanData is performed first\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('pca', TruncatedSVD(n_components=2))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to training\n",
    "clean_and_pca_pipeline.fit(sparse_train)\n",
    "# Transform the training data\n",
    "pca_sparse_train = clean_and_pca_pipeline.transform(sparse_train)\n",
    "\n",
    "pca_sparse_test = clean_and_pca_pipeline.transform(sparse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcfad5-2055-4bd5-ac33-6cda5241999f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reindexer = my_utils.Reindex(columns=[\"PC1\", \"PC2\"], names=row_names, output_folder=\"../Output\")\n",
    "train_pca_df = reindexer.transform(pca_sparse_train, train_indices, \"train\")\n",
    "test_pca_df = reindexer.transform(pca_sparse_test, test_indices, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce4aee-7198-440b-90db-f654e038c82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eda_pca = my_utils.DataEDAPCA(columns=[\"PC1\", \"PC2\"], trans=False, graphs=True)\n",
    "pca_train_df, metadata_empty, outlier_df = eda_pca.fit_transform(train_pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732968b-22a1-4c8a-9fab-057d6c4742b7",
   "metadata": {},
   "source": [
    "## Clustering on the Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0fc11-1062-4743-8fec-51d9c7c635bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_train_df = pd.read_csv('../Output/pca_train_df_without_outliers.csv', index_col = 0)\n",
    "pca_test_df =  pd.read_csv('../Output/pca_test_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e768185-a84a-4629-a547-f6ade2bfc855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Your existing code for HDBSCAN clustering\n",
    "hdbscan_params = {\n",
    "    'min_samples': [10, 30, 50, 60, 100],\n",
    "    'min_cluster_size': [100, 200, 300, 400, 500, 600],\n",
    "    'cluster_selection_method': ['eom', 'leaf'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "# Create a pipeline\n",
    "hdbscan_pipe = Pipeline([\n",
    "    (\"clusterer\", my_utils.Optimize_and_Compare_Hdbscan(hdbscan_params)),\n",
    "])\n",
    "\n",
    "results_hbd = hdbscan_pipe.fit(pca_train_df)\n",
    "best_estimator_hbd = results_hbd.named_steps['clusterer'].best_estimator\n",
    "\n",
    "try:\n",
    "    # Calculate silhouette score for test data\n",
    "    silhouette_test_hbd = silhouette_score(pca_test_df, best_estimator_hbd.fit_predict(pca_test_df))\n",
    "    print(f'Silhouette Score on test data: {silhouette_test_hbd}')\n",
    "except ValueError as e:\n",
    "    print(\"Only one cluster for my test data set. HDBSCAN does not work well for this data set\")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "silhouette_train_hbd = silhouette_score(pca_train_df, best_estimator_hbd.labels_)\n",
    "print(f'Silhouette Score on training data: {silhouette_train_hbd}')\n",
    "print(f'Time taken: {elapsed_time / 60:.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177b713-72e7-4faf-aed1-fa5b297c6519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_predictions = best_estimator_hbd.fit_predict(pca_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e3975-d4e6-4b4a-b29a-4b0c657e661e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "kmeans_params = {\n",
    "    'n_clusters': list(range(1, 10)),\n",
    "    'init': ['random', 'k-means++'],\n",
    "    'n_init': [1, 5, 10],\n",
    "    'max_iter': [300],\n",
    "    'random_state': [0]\n",
    "}\n",
    "\n",
    "# Create a pipeline\n",
    "k_means_pipe = Pipeline([\n",
    "    (\"clusterer\", my_utils.OptimizeAndCompareKMeans(kmeans_params)),\n",
    "])\n",
    "\n",
    "results = k_means_pipe.fit(pca_train_df)\n",
    "best_estimator = results.named_steps['clusterer'].best_estimator\n",
    "predictions_test = best_estimator.predict(pca_test_df)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assuming you have already fitted the pipeline and obtained the best_estimator\n",
    "\n",
    "# For training data\n",
    "silhouette_train = silhouette_score(pca_train_df, best_estimator.labels_)\n",
    "\n",
    "# For test data\n",
    "silhouette_test = silhouette_score(pca_test_df, best_estimator.predict(pca_test_df))\n",
    "\n",
    "# Stop timing\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f'Silhouette Score on training data: {silhouette_train}')\n",
    "print(f'Silhouette Score on test data: {silhouette_test}')\n",
    "print(f'Time taken: {elapsed_time / 60:.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27927a6a-82fa-41d8-ab50-3ebc71df5a65",
   "metadata": {},
   "source": [
    "I will use the Kmeans as my best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f297064-e527-4a82-b833-83fbc13e88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_df_untransposed = my_utils.create_labels_and_scoring_df(best_estimator, '../Output/best_estimator_untransposed_data_label_and_score', pca_train_df, pca_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e65cd3-c533-4b64-ac7a-fe142b255a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use your best_estimator to predict labels for the data\n",
    "cluster_labels = best_estimator.predict(pca_train_df)\n",
    "\n",
    "# Add the cluster_labels to the training data DataFrame\n",
    "pca_train_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Create a scatter plot to visualize the clustering\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', data=pca_train_df, hue='Cluster', palette='viridis', s=50, alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Clustering Visualization')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.legend(title='Cluster', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e22a4c4-a081-4085-92c8-e0c7b13a00ce",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/tuning-with-hdbscan-149865ac2970"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076af96-9d6f-4f0e-afa9-383cd9a3ea03",
   "metadata": {},
   "source": [
    "Things to consider\n",
    "\n",
    "\n",
    "1) Get the clustering to work in a pipeline\n",
    "2) Are these ok default parameters\n",
    "3) What to do with the test data\n",
    "4) Should we be using silhoute on grid search or should i be optimizaing it differently\n",
    "5) Are we normalizing it the right way? Since normalize is working along rows is this dealing with out of domain sampels? Just used standard scalar because no normalizers could work and some of them did along rows\n",
    "6) Make it so it put things in folders\n",
    "7) Quality check NAS? How do to this with a sparse matrix\n",
    "8) Do soemthing like she did in notebook 3 to label the data set\n",
    "9) Naive classifier and base? \n",
    "10) Would k-means be base what is naive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519da77-d027-45c0-a89a-fe8ea229874f",
   "metadata": {},
   "source": [
    "Transposed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300a8d0-76ef-46dd-8cc5-472d81e98d72",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ccb77b3-058c-42e7-9db5-895c2efed4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning Transposed matrix, row_names of the transposed matrix, col_names of the transposed matrix, and row_indices of transposed matrix\n"
     ]
    }
   ],
   "source": [
    "data_path = '../Output/non_zero.parquet'\n",
    "row_info_path = '../Output/row_names.csv'\n",
    "column_info_path = '../Output/col_names.csv'\n",
    "#sparse_matrix = coo_matrix((data['nonzero_elements'], (data['row_indices'], data['col_indices'])))\n",
    "sparse_matrix_trans, row_names_trans, col_names_trans, row_indices_trans= my_utils.load_data(data_path, row_info_path, column_info_path, transpose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b2ac8-84dd-456b-b02c-9c846c4317bb",
   "metadata": {},
   "source": [
    "## Demographic Feature Cleaning for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1946e8e8-018b-4a5b-bd53-9eb75839a979",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PKD2        0.106241\n",
       "PKD6        0.104167\n",
       "control4    0.099338\n",
       "control2    0.093009\n",
       "PKD1        0.090575\n",
       "PKD3        0.088687\n",
       "PKD4        0.088112\n",
       "PKD5        0.077879\n",
       "control5    0.075679\n",
       "control3    0.065193\n",
       "control1    0.062428\n",
       "PKD7        0.024428\n",
       "PKD8        0.024262\n",
       "Name: patient, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_count = metadata['patient'].value_counts(normalize=True)\n",
    "cell_count.columns = ['patient', 'percentage']\n",
    "cell_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5e7962-be67-4611-a2c6-0b6885d8330f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting metadata\n",
      "Sending balanced sample to /Output/even_distribution_sample.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksongazin/Desktop/gradyear2/Data_Mining/Assignment4/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/my_utils.py:495: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  even_distribution_sample = even_distribution_sample.append(sampled_rows)\n",
      "/Users/jacksongazin/Desktop/gradyear2/Data_Mining/Assignment4/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/my_utils.py:495: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  even_distribution_sample = even_distribution_sample.append(sampled_rows)\n",
      "/Users/jacksongazin/Desktop/gradyear2/Data_Mining/Assignment4/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/my_utils.py:495: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  even_distribution_sample = even_distribution_sample.append(sampled_rows)\n",
      "/Users/jacksongazin/Desktop/gradyear2/Data_Mining/Assignment4/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/my_utils.py:495: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  even_distribution_sample = even_distribution_sample.append(sampled_rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting training data to be balanced data set\n"
     ]
    }
   ],
   "source": [
    "split_pipeline_trans = Pipeline([\n",
    "    ('splitter', my_utils.PreserveRowIndicesSplitter(\n",
    "        test_size=0.2, random_state=42,\n",
    "        input_metadata=metadata, row_names=row_names_trans, groupby_columns=['gender', 'disease']\n",
    "    )),\n",
    "    # Add other steps in the pipeline as needed\n",
    "])\n",
    "(\n",
    "    sparse_matrix_train_trans,\n",
    "    sparse_matrix_test_trans,\n",
    "    train_row_indices_trans,\n",
    "    test_row_indices_trans\n",
    ") = split_pipeline_trans.fit_transform(sparse_matrix_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a8e7028-f3f5-4c6a-afa8-b72d9e8d6401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_and_pca_pipeline_trans = Pipeline([\n",
    "    ('cleaner', my_utils.DataClean(trans=True)),  # Specify that the data is transposed\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('pca', TruncatedSVD(n_components=2))\n",
    "])\n",
    "# Fit the pipeline to training\n",
    "clean_and_pca_pipeline_trans.fit(sparse_matrix_train_trans)\n",
    "# Transform the training data\n",
    "pca_sparse_train_trans = clean_and_pca_pipeline_trans.transform(sparse_matrix_train_trans)\n",
    "pca_sparse_test_trans= clean_and_pca_pipeline_trans.transform(sparse_matrix_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bc1af-6fe0-484d-b282-6b1ad7faba4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reindexer_trans = my_utils.Reindex(columns=[\"PC1\", \"PC2\"], names=row_names_trans, output_folder=\"../Output\", trans = True)\n",
    "train_pca_df_trans = reindexer_trans.transform(pca_sparse_train_trans,  train_row_indices_trans, \"train\")\n",
    "test_pca_df_trans = reindexer_trans.transform(pca_sparse_test_trans, test_indices_trans, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630326a-f418-4f58-bcec-4123d3586564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eda_pca = my_utils.DataEDAPCA(columns=[\"PC1\", \"PC2\"], trans = True, graphs = True)\n",
    "updated_train_pca_df_trans, metadata_removed, outlier_df_trans= eda_pca.fit_transform(train_pca_df_trans,another_df=  metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31160b-4bb3-4c60-ac8b-4bb9cb0e26af",
   "metadata": {},
   "source": [
    "Clustering on the Transposed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96013592-4483-4776-9d6d-ce4ef1dec211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pyarrow.parquet import read_table\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from memory_profiler import memory_usage\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# You can also define custom functions, classes, and other code in this module.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdff41-6c21-4429-9c99-048e528bc931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    if len(set(labels)) == 1:\n",
    "        return 0  # Silhouette score is undefined for a single cluster\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "def optimize_and_compare_hdbscan(data, hdbscan_params, alpha=0.05):\n",
    "    # Perform Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=HDBSCAN(min_cluster_size=20),\n",
    "        param_grid=hdbscan_params,\n",
    "        scoring=silhouette_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(data)\n",
    "    grid_search_estimator = grid_search.best_estimator_\n",
    "\n",
    "    # Calculate silhouette scores for the default and grid search estimators\n",
    "    default_hdbscan = HDBSCAN(min_cluster_size=20).fit(data)\n",
    "    default_labels = default_hdbscan.labels_\n",
    "    default_silhouette_score = silhouette_score(data, default_labels)\n",
    "\n",
    "    grid_search_labels = grid_search_estimator.fit_predict(data)\n",
    "    grid_search_silhouette_score = silhouette_score(data, grid_search_labels)\n",
    "\n",
    "    # Check if the grid search estimator has a higher silhouette score\n",
    "    if grid_search_silhouette_score > default_silhouette_score:\n",
    "        # Perform a two-sample t-test\n",
    "        t_stat, p_value = stats.ttest_ind(default_labels, grid_search_labels)\n",
    "\n",
    "        # Check if the p-value is less than the significance level\n",
    "        if p_value < alpha:\n",
    "            choice = \"Grid Search Estimator\"\n",
    "        else:\n",
    "            choice = \"Default Parameter\"\n",
    "    else:\n",
    "        choice = \"Default Parameter\"\n",
    "\n",
    "    # Output informative print statements\n",
    "    print(\"Default HDBSCAN Silhouette Score:\", default_silhouette_score)\n",
    "    print(\"Grid Search Estimator Silhouette Score:\", grid_search_silhouette_score)\n",
    "\n",
    "    if grid_search_silhouette_score > default_silhouette_score:\n",
    "        if p_value < alpha:\n",
    "            print(\"The difference between the two groups is statistically significant.\")\n",
    "            print(f\"Using {choice} as it performs significantly better.\")\n",
    "        else:\n",
    "            print(\"The difference between the two groups is not statistically significant.\")\n",
    "            print(f\"Using {choice} as there is no significant improvement.\")\n",
    "    else:\n",
    "        print(\"Default Parameter has a higher silhouette score. No t-test performed.\")\n",
    "\n",
    "    return choice\n",
    "\n",
    "# Define the parameter grid for HDBSCAN\n",
    "hdbscan_params = {\n",
    "    'min_samples': [10, 30, 50, 60, 100],\n",
    "    'min_cluster_size': [100, 200, 300, 400, 500, 600],\n",
    "    'cluster_selection_method': ['eom', 'leaf'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Usage example with parameters\n",
    "result = optimize_and_compare_hdbscan(updated_train_pca_df_trans, hdbscan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bffc18-714b-44ab-9ef9-cc2c21ce8ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
