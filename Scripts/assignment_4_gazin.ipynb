{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20ad65b",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe8befa-30dd-4af5-995f-a8309cdfe639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import my_utils\n",
    "import gzip\n",
    "import shutil\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd8c97-6e72-47c6-bee6-2a807d532e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_gz_file = '../Output/GSE185948_count_RNA.rds.gz'\n",
    "output_rds_file = '../Output/data_for_r.rds'\n",
    "\n",
    "# Open the compressed file and extract it\n",
    "with gzip.open(input_gz_file, 'rb') as f_in, open(output_rds_file, 'wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f'{input_gz_file} has been successfully uncompressed to {output_rds_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77773596-d217-481d-90eb-cff1fe9563ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_gz_file = '../Output/GSE185948_metadata_RNA.csv.gz'\n",
    "output_csv_file = '../Output/uncompressed_metadata.csv'\n",
    "\n",
    "# Open the compressed file and extract it\n",
    "with gzip.open(input_gz_file, 'rb') as f_in, open(output_csv_file, 'wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f'{input_gz_file} has been successfully uncompressed to {output_csv_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd10524-abd6-4eda-8ea8-9d53cff8d84c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = '../Output'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d033d150-def7-422c-b556-cfadb89c76db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('../Output/uncompressed_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd6966f-06c4-42fa-8742-c0702d7602b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>barcode</th>\n",
       "      <th>patient</th>\n",
       "      <th>gender</th>\n",
       "      <th>disease</th>\n",
       "      <th>celltype</th>\n",
       "      <th>nCount_RNA</th>\n",
       "      <th>nFeature_RNA</th>\n",
       "      <th>UMAP_1</th>\n",
       "      <th>UMAP_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PKD_ACACGCGGTATCGGTT-1_1</td>\n",
       "      <td>ACACGCGGTATCGGTT-1</td>\n",
       "      <td>PKD1</td>\n",
       "      <td>female</td>\n",
       "      <td>PKD</td>\n",
       "      <td>TAL1</td>\n",
       "      <td>1234.684629</td>\n",
       "      <td>1250</td>\n",
       "      <td>-7.416020</td>\n",
       "      <td>-6.008874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PKD_ACACGCGGTTTGGCTA-1_1</td>\n",
       "      <td>ACACGCGGTTTGGCTA-1</td>\n",
       "      <td>PKD1</td>\n",
       "      <td>female</td>\n",
       "      <td>PKD</td>\n",
       "      <td>PT2</td>\n",
       "      <td>1865.542588</td>\n",
       "      <td>1650</td>\n",
       "      <td>2.499409</td>\n",
       "      <td>-6.587287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PKD_ACACGCGTCATGTCTT-1_1</td>\n",
       "      <td>ACACGCGTCATGTCTT-1</td>\n",
       "      <td>PKD1</td>\n",
       "      <td>female</td>\n",
       "      <td>PKD</td>\n",
       "      <td>CNT_PC</td>\n",
       "      <td>1812.700523</td>\n",
       "      <td>1419</td>\n",
       "      <td>-2.254505</td>\n",
       "      <td>8.526364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PKD_ACACTGAAGACCCTTA-1_1</td>\n",
       "      <td>ACACTGAAGACCCTTA-1</td>\n",
       "      <td>PKD1</td>\n",
       "      <td>female</td>\n",
       "      <td>PKD</td>\n",
       "      <td>FIB</td>\n",
       "      <td>978.772591</td>\n",
       "      <td>1089</td>\n",
       "      <td>9.949437</td>\n",
       "      <td>2.086737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PKD_ACACTGAAGCGACAGT-1_1</td>\n",
       "      <td>ACACTGAAGCGACAGT-1</td>\n",
       "      <td>PKD1</td>\n",
       "      <td>female</td>\n",
       "      <td>PKD</td>\n",
       "      <td>PT1</td>\n",
       "      <td>2361.558871</td>\n",
       "      <td>2013</td>\n",
       "      <td>7.886921</td>\n",
       "      <td>-8.587954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102705</th>\n",
       "      <td>Cont_TTTGTTGCAGTTAAAG-1_5</td>\n",
       "      <td>TTTGTTGCAGTTAAAG-1</td>\n",
       "      <td>control5</td>\n",
       "      <td>female</td>\n",
       "      <td>control</td>\n",
       "      <td>DCT</td>\n",
       "      <td>2177.456940</td>\n",
       "      <td>1537</td>\n",
       "      <td>-8.038441</td>\n",
       "      <td>6.705230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102706</th>\n",
       "      <td>Cont_TTTGTTGCATATTCGG-1_5</td>\n",
       "      <td>TTTGTTGCATATTCGG-1</td>\n",
       "      <td>control5</td>\n",
       "      <td>female</td>\n",
       "      <td>control</td>\n",
       "      <td>PT1</td>\n",
       "      <td>1116.704812</td>\n",
       "      <td>984</td>\n",
       "      <td>5.658295</td>\n",
       "      <td>-6.785034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102707</th>\n",
       "      <td>Cont_TTTGTTGGTACGATTC-1_5</td>\n",
       "      <td>TTTGTTGGTACGATTC-1</td>\n",
       "      <td>control5</td>\n",
       "      <td>female</td>\n",
       "      <td>control</td>\n",
       "      <td>DCT</td>\n",
       "      <td>2156.953541</td>\n",
       "      <td>1421</td>\n",
       "      <td>-8.645567</td>\n",
       "      <td>6.688504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102708</th>\n",
       "      <td>Cont_TTTGTTGGTGCGTTTA-1_5</td>\n",
       "      <td>TTTGTTGGTGCGTTTA-1</td>\n",
       "      <td>control5</td>\n",
       "      <td>female</td>\n",
       "      <td>control</td>\n",
       "      <td>CNT_PC</td>\n",
       "      <td>1553.371592</td>\n",
       "      <td>1209</td>\n",
       "      <td>-0.795698</td>\n",
       "      <td>7.423819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102709</th>\n",
       "      <td>Cont_TTTGTTGTCTGCGGAC-1_5</td>\n",
       "      <td>TTTGTTGTCTGCGGAC-1</td>\n",
       "      <td>control5</td>\n",
       "      <td>female</td>\n",
       "      <td>control</td>\n",
       "      <td>ENDO</td>\n",
       "      <td>1106.715246</td>\n",
       "      <td>917</td>\n",
       "      <td>2.259402</td>\n",
       "      <td>3.275315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102710 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name             barcode   patient  gender  \\\n",
       "0        PKD_ACACGCGGTATCGGTT-1_1  ACACGCGGTATCGGTT-1      PKD1  female   \n",
       "1        PKD_ACACGCGGTTTGGCTA-1_1  ACACGCGGTTTGGCTA-1      PKD1  female   \n",
       "2        PKD_ACACGCGTCATGTCTT-1_1  ACACGCGTCATGTCTT-1      PKD1  female   \n",
       "3        PKD_ACACTGAAGACCCTTA-1_1  ACACTGAAGACCCTTA-1      PKD1  female   \n",
       "4        PKD_ACACTGAAGCGACAGT-1_1  ACACTGAAGCGACAGT-1      PKD1  female   \n",
       "...                           ...                 ...       ...     ...   \n",
       "102705  Cont_TTTGTTGCAGTTAAAG-1_5  TTTGTTGCAGTTAAAG-1  control5  female   \n",
       "102706  Cont_TTTGTTGCATATTCGG-1_5  TTTGTTGCATATTCGG-1  control5  female   \n",
       "102707  Cont_TTTGTTGGTACGATTC-1_5  TTTGTTGGTACGATTC-1  control5  female   \n",
       "102708  Cont_TTTGTTGGTGCGTTTA-1_5  TTTGTTGGTGCGTTTA-1  control5  female   \n",
       "102709  Cont_TTTGTTGTCTGCGGAC-1_5  TTTGTTGTCTGCGGAC-1  control5  female   \n",
       "\n",
       "        disease celltype   nCount_RNA  nFeature_RNA    UMAP_1    UMAP_2  \n",
       "0           PKD     TAL1  1234.684629          1250 -7.416020 -6.008874  \n",
       "1           PKD      PT2  1865.542588          1650  2.499409 -6.587287  \n",
       "2           PKD   CNT_PC  1812.700523          1419 -2.254505  8.526364  \n",
       "3           PKD      FIB   978.772591          1089  9.949437  2.086737  \n",
       "4           PKD      PT1  2361.558871          2013  7.886921 -8.587954  \n",
       "...         ...      ...          ...           ...       ...       ...  \n",
       "102705  control      DCT  2177.456940          1537 -8.038441  6.705230  \n",
       "102706  control      PT1  1116.704812           984  5.658295 -6.785034  \n",
       "102707  control      DCT  2156.953541          1421 -8.645567  6.688504  \n",
       "102708  control   CNT_PC  1553.371592          1209 -0.795698  7.423819  \n",
       "102709  control     ENDO  1106.715246           917  2.259402  3.275315  \n",
       "\n",
       "[102710 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12762e-d950-4f0d-88ae-cd32ec8e43ca",
   "metadata": {},
   "source": [
    "R code below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea593dcf-f6ef-476e-97fa-5527db6fb58a",
   "metadata": {},
   "source": [
    "filename = file.choose()\n",
    "data = readRDS(filename)\n",
    "data\n",
    "sparse_matrix <- data\n",
    "\n",
    "num_columns <- ncol(sparse_matrix)\n",
    "# Get the nonzero elements and their indices\n",
    "nonzero_elements <- sparse_matrix@x\n",
    "row_indices <- sparse_matrix@i\n",
    "col_indices <- rep(1:num_columns, times = diff(sparse_matrix@p))  # Use sparse_matrix@j for column indices\n",
    "\n",
    "\n",
    "nonzero_data <- data.frame(row_indices, col_indices, nonzero_elements)\n",
    "\n",
    "col_names <- data.frame(sparse_matrix@Dimnames[2])\n",
    "nrow(col_names)\n",
    "row_names <- data.frame((sparse_matrix@Dimnames[1]))\n",
    "\n",
    "write.csv(row_names, 'row_names.csv')\n",
    "write.csv(col_names, 'col_names.csv')\n",
    "\n",
    "\n",
    "# Save the i, j information to a Parquet file\n",
    "write_parquet(nonzero_data, \"non_zero.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c87489-add6-4698-bc23-884af1dcaf8c",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d881cd-1a18-43f9-84d1-ff85e77ef9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade --no-deps memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d363c1-327c-4470-b520-af645a113edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '../Output/non_zero.parquet'\n",
    "row_info_path = '../Output/row_names.csv'\n",
    "column_info_path = '../Output/col_names.csv'\n",
    "#sparse_matrix = coo_matrix((data['nonzero_elements'], (data['row_indices'], data['col_indices'])))\n",
    "sparse_matrix, column_names, row_names, row_indices= my_utils.load_data(data_path, row_info_path, column_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb6927-abb7-4961-a2b4-3447d71d8234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_pipeline = Pipeline([\n",
    "    ('splitter', my_utils.SparseTrainTestSplit(test_size=0.2, random_state=42, row_indices = row_indices)),\n",
    "    # Add other steps in the pipeline as needed\n",
    "])\n",
    "\n",
    "# Fit and transform the pipeline\n",
    "sparse_train, sparse_test, train_indices, test_indices = split_pipeline.fit_transform(sparse_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f17eec-f7e8-4790-b8f1-bd978367b2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfeb12c-ab56-424c-a09a-62dd30fc2494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_and_pca_pipeline = Pipeline([\n",
    "    ('cleaner', my_utils.DataClean()),  # CleanData is performed first\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('pca', TruncatedSVD(n_components=2))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to training\n",
    "clean_and_pca_pipeline.fit(sparse_train)\n",
    "# Transform the training data\n",
    "pca_sparse_train = clean_and_pca_pipeline.transform(sparse_train)\n",
    "\n",
    "pca_sparse_test = clean_and_pca_pipeline.transform(sparse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcfad5-2055-4bd5-ac33-6cda5241999f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reindexer = my_utils.Reindex(columns=[\"PC1\", \"PC2\"], names=row_names, output_folder=\"../Output\")\n",
    "train_pca_df = reindexer.transform(pca_sparse_train, train_indices, \"train\")\n",
    "test_pca_df = reindexer.transform(pca_sparse_test, test_indices, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce4aee-7198-440b-90db-f654e038c82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eda_pca = my_utils.DataEDAPCA(columns=[\"PC1\", \"PC2\"], trans = False)\n",
    "pca_train_df, metadata_empty, outlier_df = eda_pca.fit_transform(train_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ea474-5cb3-4741-b6b4-dadeb3dd8aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732968b-22a1-4c8a-9fab-057d6c4742b7",
   "metadata": {},
   "source": [
    "## Clustering on the Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b0fc11-1062-4743-8fec-51d9c7c635bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_train_df = pd.read_csv('../Output/pca_train_df_without_outliers.csv', index_col = 0)\n",
    "pca_test_df =  pd.read_csv('../Output/pca_test_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddcd827-c720-4a8e-b0de-e3307758c645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    if len(set(labels)) == 1:\n",
    "        return 0  # Silhouette score is undefined for a single cluster\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "def optimize_and_compare_hdbscan(data, hdbscan_params, alpha=0.05):\n",
    "    # Perform Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=HDBSCAN(min_cluster_size=20),\n",
    "        param_grid=hdbscan_params,\n",
    "        scoring=silhouette_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(data)\n",
    "    grid_search_estimator = grid_search.best_estimator_\n",
    "\n",
    "    # Calculate silhouette scores for the default and grid search estimators\n",
    "    default_hdbscan = HDBSCAN(min_cluster_size=20).fit(data)\n",
    "    default_labels = default_hdbscan.labels_\n",
    "    default_silhouette_score = silhouette_score(data, default_labels)\n",
    "\n",
    "    grid_search_labels = grid_search_estimator.fit_predict(data)\n",
    "    grid_search_silhouette_score = silhouette_score(data, grid_search_labels)\n",
    "\n",
    "    # Check if the grid search estimator has a higher silhouette score\n",
    "    if grid_search_silhouette_score > default_silhouette_score:\n",
    "        # Perform a two-sample t-test\n",
    "        t_stat, p_value = stats.ttest_ind(default_labels, grid_search_labels)\n",
    "\n",
    "        # Check if the p-value is less than the significance level\n",
    "        if p_value < alpha:\n",
    "            choice = \"Grid Search Estimator\"\n",
    "        else:\n",
    "            choice = \"Default Parameter\"\n",
    "    else:\n",
    "        choice = \"Default Parameter\"\n",
    "\n",
    "    # Output informative print statements\n",
    "    print(\"Default HDBSCAN Silhouette Score:\", default_silhouette_score)\n",
    "    print(\"Grid Search Estimator Silhouette Score:\", grid_search_silhouette_score)\n",
    "\n",
    "    if grid_search_silhouette_score > default_silhouette_score:\n",
    "        if p_value < alpha:\n",
    "            print(\"The difference between the two groups is statistically significant.\")\n",
    "            print(f\"Using {choice} as it performs significantly better.\")\n",
    "        else:\n",
    "            print(\"The difference between the two groups is not statistically significant.\")\n",
    "            print(f\"Using {choice} as there is no significant improvement.\")\n",
    "    else:\n",
    "        print(\"Default Parameter has a higher silhouette score. No t-test performed.\")\n",
    "\n",
    "    return choice\n",
    "\n",
    "# Define the parameter grid for HDBSCAN\n",
    "hdbscan_params = {\n",
    "    'min_samples': [10, 30, 50, 60, 100],\n",
    "    'min_cluster_size': [100, 200, 300, 400, 500, 600],\n",
    "    'cluster_selection_method': ['eom', 'leaf'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Usage example with parameters\n",
    "result = optimize_and_compare_hdbscan(train_without_outliers, hdbscan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f87fb0-c303-45a4-89c1-be75cc3bcae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f788051-87a2-47e4-80ac-8fcfebf441e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a9e3975-d4e6-4b4a-b29a-4b0c657e661e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default KMeans Silhouette Score: 0.8797200918856573\n",
      "Grid Search Estimator Silhouette Score: 0.8799544859177173\n",
      "The difference between the two groups is not statistically significant.\n",
      "Using Default KMeans Estimator as there is no significant improvement using a threshold of alpha = .05.\n",
      "Silhouette Score on training data: 0.8797200918856573\n",
      "Silhouette Score on test data: 0.8752799631951433\n"
     ]
    }
   ],
   "source": [
    "kmeans_params = {\n",
    "    'n_clusters': list(range(1, 10)),\n",
    "    'init': ['random', 'k-means++'],\n",
    "    'n_init': [1, 5, 10],\n",
    "    'max_iter': [300],\n",
    "    'random_state': [0]\n",
    "}\n",
    "\n",
    "# Create a pipeline\n",
    "k_means_pipe = Pipeline([\n",
    "    (\"clusterer\", my_utils.OptimizeAndCompareKMeans(kmeans_params)),\n",
    "])\n",
    "\n",
    "results = k_means_pipe.fit(pca_train_df)\n",
    "best_estimator = results.named_steps['clusterer'].best_estimator\n",
    "predictions_test = best_estimator.predict(pca_test_df)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assuming you have already fitted the pipeline and obtained the best_estimator\n",
    "\n",
    "# For training data\n",
    "silhouette_train = silhouette_score(pca_train_df, best_estimator.labels_)\n",
    "\n",
    "# For test data\n",
    "silhouette_test = silhouette_score(pca_test_df, best_estimator.predict(pca_test_df))\n",
    "\n",
    "print(f'Silhouette Score on training data: {silhouette_train}')\n",
    "print(f'Silhouette Score on test data: {silhouette_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933f9ac-959c-4c1a-b671-a1177391c638",
   "metadata": {},
   "source": [
    "Silhoute Score is robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e22a4c4-a081-4085-92c8-e0c7b13a00ce",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/tuning-with-hdbscan-149865ac2970"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076af96-9d6f-4f0e-afa9-383cd9a3ea03",
   "metadata": {},
   "source": [
    "Things to consider\n",
    "\n",
    "\n",
    "1) Get the clustering to work in a pipeline\n",
    "2) Are these ok default parameters\n",
    "3) What to do with the test data\n",
    "4) Should we be using silhoute on grid search or should i be optimizaing it differently\n",
    "5) Are we normalizing it the right way? Since normalize is working along rows is this dealing with out of domain sampels? Just used standard scalar because no normalizers could work and some of them did along rows\n",
    "6) Make it so it put things in folders\n",
    "7) Quality check NAS? How do to this with a sparse matrix\n",
    "8) Do soemthing like she did in notebook 3 to label the data set\n",
    "9) Naive classifier and base? \n",
    "10) Would k-means be base what is naive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519da77-d027-45c0-a89a-fe8ea229874f",
   "metadata": {},
   "source": [
    "Transposed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300a8d0-76ef-46dd-8cc5-472d81e98d72",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb77b3-058c-42e7-9db5-895c2efed4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '../Output/non_zero.parquet'\n",
    "row_info_path = '../Output/row_names.csv'\n",
    "column_info_path = '../Output/col_names.csv'\n",
    "#sparse_matrix = coo_matrix((data['nonzero_elements'], (data['row_indices'], data['col_indices'])))\n",
    "sparse_matrix_trans, row_names_trans, col_names_trans, row_indices_trans= my_utils.load_data(data_path, row_info_path, column_info_path, transpose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b8e4aa-3f31-4462-947a-7094119825ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_pipeline_trans = Pipeline([\n",
    "    ('splitter', my_utils.SparseTrainTestSplit(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        row_indices=row_indices_trans\n",
    "    )),\n",
    "    # Add other steps in the pipeline as needed\n",
    "])\n",
    "\n",
    "# Fit and transform the pipeline\n",
    "(\n",
    "    sparse_train_trans,\n",
    "    sparse_test_trans,\n",
    "    train_indices_trans,\n",
    "    test_indices_trans\n",
    ") = split_pipeline_trans.fit_transform(sparse_matrix_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e7028-f3f5-4c6a-afa8-b72d9e8d6401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_and_pca_pipeline_trans = Pipeline([\n",
    "    ('cleaner', my_utils.DataClean()),  # CleanData is performed first\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('pca', TruncatedSVD(n_components=2))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to training\n",
    "clean_and_pca_pipeline_trans.fit(sparse_train_trans)\n",
    "# Transform the training data\n",
    "pca_sparse_train_trans = clean_and_pca_pipeline_trans.transform(sparse_train_trans)\n",
    "pca_sparse_test_trans= clean_and_pca_pipeline_trans.transform(sparse_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bc1af-6fe0-484d-b282-6b1ad7faba4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reindexer_trans = my_utils.Reindex(columns=[\"PC1\", \"PC2\"], names=row_names_trans, output_folder=\"../Output\")\n",
    "train_pca_df_trans = reindexer_trans.transform(pca_sparse_train_trans, train_indices_trans, \"train\")\n",
    "test_pca_df_trans = reindexer_trans.transform(pca_sparse_test_trans, test_indices_trans, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9139a-a229-43cf-ab0d-d7ff5e3fbffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pca_df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630326a-f418-4f58-bcec-4123d3586564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eda_pca = my_utils.DataEDAPCA(columns=[\"PC1\", \"PC2\"])\n",
    "updated_train_pca_df_trans, metadata_removed, outlier_df_trans= eda_pca.fit_transform(train_pca_df_trans,another_df=  metadata, trans = \"trans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31160b-4bb3-4c60-ac8b-4bb9cb0e26af",
   "metadata": {},
   "source": [
    "Clustering on the Transposed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96013592-4483-4776-9d6d-ce4ef1dec211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pyarrow.parquet import read_table\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from memory_profiler import memory_usage\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# You can also define custom functions, classes, and other code in this module.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdff41-6c21-4429-9c99-048e528bc931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    if len(set(labels)) == 1:\n",
    "        return 0  # Silhouette score is undefined for a single cluster\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "def optimize_and_compare_hdbscan(data, hdbscan_params, alpha=0.05):\n",
    "    # Perform Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=HDBSCAN(min_cluster_size=20),\n",
    "        param_grid=hdbscan_params,\n",
    "        scoring=silhouette_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(data)\n",
    "    grid_search_estimator = grid_search.best_estimator_\n",
    "\n",
    "    # Calculate silhouette scores for the default and grid search estimators\n",
    "    default_hdbscan = HDBSCAN(min_cluster_size=20).fit(data)\n",
    "    default_labels = default_hdbscan.labels_\n",
    "    default_silhouette_score = silhouette_score(data, default_labels)\n",
    "\n",
    "    grid_search_labels = grid_search_estimator.fit_predict(data)\n",
    "    grid_search_silhouette_score = silhouette_score(data, grid_search_labels)\n",
    "\n",
    "    # Check if the grid search estimator has a higher silhouette score\n",
    "    if grid_search_silhouette_score > default_silhouette_score:\n",
    "        # Perform a two-sample t-test\n",
    "        t_stat, p_value = stats.ttest_ind(default_labels, grid_search_labels)\n",
    "\n",
    "        # Check if the p-value is less than the significance level\n",
    "        if p_value < alpha:\n",
    "            choice = \"Grid Search Estimator\"\n",
    "        else:\n",
    "            choice = \"Default Parameter\"\n",
    "    else:\n",
    "        choice = \"Default Parameter\"\n",
    "\n",
    "    # Output informative print statements\n",
    "    print(\"Default HDBSCAN Silhouette Score:\", default_silhouette_score)\n",
    "    print(\"Grid Search Estimator Silhouette Score:\", grid_search_silhouette_score)\n",
    "\n",
    "    if grid_search_silhouette_score > default_silhouette_score:\n",
    "        if p_value < alpha:\n",
    "            print(\"The difference between the two groups is statistically significant.\")\n",
    "            print(f\"Using {choice} as it performs significantly better.\")\n",
    "        else:\n",
    "            print(\"The difference between the two groups is not statistically significant.\")\n",
    "            print(f\"Using {choice} as there is no significant improvement.\")\n",
    "    else:\n",
    "        print(\"Default Parameter has a higher silhouette score. No t-test performed.\")\n",
    "\n",
    "    return choice\n",
    "\n",
    "# Define the parameter grid for HDBSCAN\n",
    "hdbscan_params = {\n",
    "    'min_samples': [10, 30, 50, 60, 100],\n",
    "    'min_cluster_size': [100, 200, 300, 400, 500, 600],\n",
    "    'cluster_selection_method': ['eom', 'leaf'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Usage example with parameters\n",
    "result = optimize_and_compare_hdbscan(updated_train_pca_df_trans, hdbscan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bffc18-714b-44ab-9ef9-cc2c21ce8ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
