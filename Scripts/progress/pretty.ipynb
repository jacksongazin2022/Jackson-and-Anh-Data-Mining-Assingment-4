{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "+ pickle pipeline\n",
    "+ save output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pyarrow.parquet import read_table\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from memory_profiler import memory_usage\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "import my_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(input, output):\n",
    "    with gzip.open(input, 'rb') as f_in, open(output, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    print(f'{input} has been successfully uncompressed to {output}.')\n",
    "\n",
    "def load_data(data_path, row_info_path, column_info_path):\n",
    "    # Load non_zero parquet data\n",
    "    table = read_table(data_path)\n",
    "    nonzero_data = table.to_pandas()\n",
    "    \n",
    "    # Adjust column indices to be 0-based\n",
    "    nonzero_data['col_indices'] = nonzero_data['col_indices'] - 1\n",
    "    \n",
    "    # Load row and column index info\n",
    "    rows = pd.read_csv(row_info_path)\n",
    "    row_names = rows.iloc[:, 1].to_list()\n",
    "    \n",
    "    columns = pd.read_csv(column_info_path)\n",
    "    column_names = columns.iloc[:, 1].to_list()\n",
    "    \n",
    "    # Convert the sparse matrix to a dense DataFrame\n",
    "    sparse_matrix = coo_matrix(\n",
    "        (nonzero_data['nonzero_elements'], (nonzero_data['row_indices'], nonzero_data['col_indices'])),\n",
    "        shape=(len(row_names), len(column_names))\n",
    "    )\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    print('Returning sparse_matrix, column_names, and row_names')\n",
    "    \n",
    "    return sparse_matrix, column_names, row_names\n",
    "\n",
    "\n",
    "class SparseTrainTestSplit(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, test_size=0.2, random_state=None):\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Not needed for this class\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Split the data using train_test_split\n",
    "        sparse_train, sparse_test, train_indices, test_indices = train_test_split(\n",
    "            X, row_indices, test_size=self.test_size, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # Convert the split data back to coo_matrix\n",
    "        sparse_train = coo_matrix(sparse_train)\n",
    "        sparse_test = coo_matrix(sparse_test)\n",
    "\n",
    "        return sparse_train, sparse_test, train_indices, test_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClean:\n",
    "    def __init__(self, remove_by_column=True):\n",
    "        self.remove_by_column = remove_by_column\n",
    "        self.output_folder = \"../output\"\n",
    "        \n",
    "        # Create the output folder if it doesn't exist\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Check if there are NaN values in the data\n",
    "        self.has_nan_values = np.isnan(X.data).any()\n",
    "\n",
    "        with open(os.path.join(self.output_folder, \"na_info.txt\"), \"w\") as f:\n",
    "            if self.has_nan_values:\n",
    "                # Find rows and columns with NaN values\n",
    "                self.rows_with_nan = self.find_rows_with_nan(X)\n",
    "                self.columns_with_nan = self.find_columns_with_nan(X)\n",
    "\n",
    "                f.write(\"Rows with NaN Values:\\n\")\n",
    "                f.write(\", \".join(map(str, self.rows_with_nan)))\n",
    "                f.write(\"\\nTotal Rows with NaN Values: {}\\n\".format(len(self.rows_with_nan)))\n",
    "\n",
    "                f.write(\"\\nColumns with NaN Values:\\n\")\n",
    "                f.write(\", \".join(map(str, self.columns_with_nan)))\n",
    "                f.write(\"\\nTotal Columns with NaN Values: {}\\n\".format(len(self.columns_with_nan)))\n",
    "            else:\n",
    "                f.write(\"No NaN Values Found in the Data.\\n\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.has_nan_values:\n",
    "            if self.remove_by_column:\n",
    "                X = self._remove_nan_by_column(X)\n",
    "            else:\n",
    "                X = self._remove_nan_by_row(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _remove_nan_by_column(self, X):\n",
    "        nan_mask = np.isnan(X.data)\n",
    "        valid_columns = np.unique(X.col[~nan_mask])\n",
    "        X = X.tocsc()[:, valid_columns].tocoo()\n",
    "        return X\n",
    "\n",
    "    def _remove_nan_by_row(self, X):\n",
    "        if not isinstance(X, coo_matrix):\n",
    "            raise ValueError(\"Input must be a sparse COO matrix.\")\n",
    "    \n",
    "        non_nan_rows = ~np.isnan(X.sum(axis=1).A.ravel())\n",
    "        X = X.tocsr()[non_nan_rows].tocoo()\n",
    "        return X\n",
    "\n",
    "    def find_rows_with_nan(self, X):\n",
    "        nan_mask = np.isnan(X.data)\n",
    "        rows_with_nan = np.unique(X.row[nan_mask])\n",
    "        return rows_with_nan\n",
    "\n",
    "    def find_columns_with_nan(self, X):\n",
    "        nan_mask = np.isnan(X.data)\n",
    "        columns_with_nan = np.unique(X.col[nan_mask])\n",
    "        return columns_with_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(df, indices, columns, names):\n",
    "    row_names_indices = [names[i] for i in indices]\n",
    "    df_with_indices = pd.DataFrame(df, columns=columns, index=row_names_indices)\n",
    "    return df_with_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEDAPCA:\n",
    "    def __init__(self, columns, z_threshold=10):\n",
    "        self.columns = columns\n",
    "        self.z_threshold = z_threshold\n",
    "        self.removed_indices = None\n",
    "\n",
    "    def plot_box_and_scatter(self, X):\n",
    "        # Create a boxplot of the specified columns\n",
    "        X[self.columns].boxplot()\n",
    "        plt.title(\"Boxplot of Selected Columns\")\n",
    "        plt.show()\n",
    "\n",
    "        # Create a scatterplot of the specified columns\n",
    "        plt.scatter(X[self.columns[0]], X[self.columns[1]])\n",
    "        plt.xlabel(self.columns[0])\n",
    "        plt.ylabel(self.columns[1])\n",
    "        plt.title(\"Scatterplot of Selected Columns\")\n",
    "        plt.show()\n",
    "\n",
    "    def remove_outliers(self, X):\n",
    "        # Calculate Z-scores for the specified columns\n",
    "        z_scores = np.abs(stats.zscore(X[self.columns]))\n",
    "        z_scores = z_scores.to_numpy()  # Convert to a NumPy array\n",
    "        print(\"Z-scores:\")\n",
    "        print(z_scores)\n",
    "        print(\"Z-threshold:\", self.z_threshold)\n",
    "\n",
    "        # Find the indices of rows with Z-scores exceeding the threshold\n",
    "        outlier_indices = np.argwhere(z_scores > self.z_threshold)\n",
    "        outlier_indices = outlier_indices.ravel()\n",
    "        print(\"Outlier Indices:\", outlier_indices)\n",
    "\n",
    "        # Drop the rows with high Z-scores\n",
    "        X.drop(X.index[outlier_indices], inplace=True)\n",
    "        self.removed_indices = outlier_indices\n",
    "\n",
    "    def fit_transform(self, X, another_df=None):\n",
    "        print(\"Shape before:\", X.shape)\n",
    "\n",
    "        # Step 1: Display initial boxplot and scatterplot\n",
    "        self.plot_box_and_scatter(X)\n",
    "\n",
    "        # Step 2: Notify rows with high Z-scores and remove them\n",
    "        self.remove_outliers(X)\n",
    "\n",
    "        # Step 3: If another_df is provided, subset it using the same indices\n",
    "        removed_metadata = None\n",
    "        if another_df is not None:\n",
    "            removed_metadata = another_df.iloc[self.removed_indices]\n",
    "\n",
    "        # Step 4: Display boxplot and scatterplot of the updated data\n",
    "        self.plot_box_and_scatter(X)\n",
    "\n",
    "        print(\"Shape after:\", X.shape)\n",
    "\n",
    "        return X, removed_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_and_compare_kmeans(data, kmeans_params, alpha=0.05):\n",
    "    # Perform Grid Search\n",
    "    grid = GridSearchCV(KMeans(), kmeans_params, cv=3, refit=True)\n",
    "    grid.fit(data)\n",
    "    grid_search_estimator = grid.best_estimator_\n",
    "\n",
    "    # Calculate silhouette scores\n",
    "    default_kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(data)\n",
    "    default_silhouette_score = silhouette_score(data, default_kmeans.labels_)\n",
    "    grid_search_silhouette_score = silhouette_score(data, grid_search_estimator.labels_)\n",
    "\n",
    "    # Perform a two-sample t-test only if Grid Search performs better\n",
    "    if grid_search_silhouette_score > default_silhouette_score:\n",
    "        t_stat, p_value = stats.ttest_ind(default_kmeans.labels_, grid_search_estimator.labels_)\n",
    "\n",
    "        # Set the default choice to \"Grid Search Estimator\"\n",
    "        choice = \"KMeans Estimator\"\n",
    "\n",
    "        # Output informative print statements\n",
    "        print(\"Default KMeans Silhouette Score:\", default_silhouette_score)\n",
    "        print(\"Grid Search Estimator Silhouette Score:\", grid_search_silhouette_score)\n",
    "\n",
    "        if p_value < alpha:\n",
    "            choice = \"Grid Search Estimator\"\n",
    "            print(\"The difference between the two groups is statistically significant.\")\n",
    "            print(f\"Using {choice} as it performs significantly better using a threshold of alpha = .05 .\")\n",
    "            return choice, grid_search_estimator\n",
    "        else:\n",
    "            print(\"The difference between the two groups is not statistically significant.\")\n",
    "            print(f\"Using {choice} as there is no significant improvement using a threshold of alpha = .05.\")\n",
    "            return choice, default_kmeans\n",
    "    else:\n",
    "        choice = \"Default Parameter\"\n",
    "        print(\"Grid Search Estimator Silhouette Score:\", grid_search_silhouette_score)\n",
    "        print(\"Default KMeans Silhouette Score:\", default_silhouette_score)\n",
    "        print(\"Default Parameter has a higher Silhouette Score.\")\n",
    "        print(\"Using Default Parameter as it performs better based on Silhouette Score.\")\n",
    "        return choice, default_kmeans\n",
    "\n",
    "     # Return both choice and the chosen KMeans clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_in_clusters(data, clusterer, n_neighbors=20, contamination=0.1):\n",
    "    # Fit the Local Outlier Factor (LOF) model on the data\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "    outliers = lof.fit_predict(data)\n",
    "\n",
    "    # Create a DataFrame to store cluster labels and outlier labels\n",
    "    cluster_outliers = pd.DataFrame({'Cluster': clusterer.labels_, 'Outlier': outliers})\n",
    "\n",
    "    # Print the number of data points and percentage of outliers in each cluster\n",
    "    cluster_info = cluster_outliers.groupby('Cluster').agg(DataPoints=('Cluster', 'count'),\n",
    "                                                          PercentageOutliers=('Outlier', lambda x: (x == -1).mean() * 100))\n",
    "    print(\"Cluster Information:\")\n",
    "    print(cluster_info)\n",
    "\n",
    "    return cluster_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_plot(df, label, cluster_name):\n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Create a scatter plot to visualize the clustering\n",
    "    sns.scatterplot(x='PC1', y='PC2', data=df, hue=label, palette='viridis', s=50, alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('Clustering Visualization for' + cluster_name)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.legend(title='Cluster', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    if len(set(labels)) == 1:\n",
    "        return 0  # Silhouette score is undefined for a single cluster\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "def optimize_and_compare_hdbscan(data, hdbscan_params, alpha=0.05):\n",
    "    # Perform Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=HDBSCAN(min_cluster_size=20),\n",
    "        param_grid=hdbscan_params,\n",
    "        scoring=silhouette_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(data)\n",
    "    grid_search_estimator = grid_search.best_estimator_\n",
    "\n",
    "    # Calculate silhouette scores for the default and grid search estimators\n",
    "    default_hdbscan = HDBSCAN(min_cluster_size=20).fit(data)\n",
    "    default_labels = default_hdbscan.labels_\n",
    "    default_silhouette_score = silhouette_score(data, default_labels)\n",
    "\n",
    "    grid_search_labels = grid_search_estimator.fit_predict(data)\n",
    "    grid_search_silhouette_score = silhouette_score(data, grid_search_labels)\n",
    "\n",
    "    # Check if the grid search estimator has a higher silhouette score\n",
    "    if grid_search_silhouette_score > default_silhouette_score:\n",
    "        # Perform a two-sample t-test\n",
    "        t_stat, p_value = stats.ttest_ind(default_labels, grid_search_labels)\n",
    "\n",
    "        # Check if the p-value is less than the significance level\n",
    "        if p_value < alpha:\n",
    "            choice = \"Grid Search Estimator\"\n",
    "        else:\n",
    "            choice = \"Default Parameter\"\n",
    "    else:\n",
    "        choice = \"Default Parameter\"\n",
    "\n",
    "    # Output informative print statements\n",
    "    print(\"Default HDBSCAN Silhouette Score:\", default_silhouette_score)\n",
    "    print(\"Grid Search Estimator Silhouette Score:\", grid_search_silhouette_score)\n",
    "\n",
    "    if grid_search_silhouette_score > default_silhouette_score:\n",
    "        if p_value < alpha:\n",
    "            print(\"The difference between the two groups is statistically significant.\")\n",
    "            print(f\"Using {choice} as it performs significantly better.\")\n",
    "        else:\n",
    "            print(\"The difference between the two groups is not statistically significant.\")\n",
    "            print(f\"Using {choice} as there is no significant improvement.\")\n",
    "    else:\n",
    "        print(\"Default Parameter has a higher silhouette score. No t-test performed.\")\n",
    "\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip .gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file\n",
    "unzip('GSE185948_count_RNA.rds.gz', 'data_for_r.rds')\n",
    "unzip('GSE185948_metadata_RNA.csv.gz', 'uncompressed_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R code to read RDS file and convert to a paquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = file.choose()\n",
    "data = readRDS(filename)\n",
    "data\n",
    "sparse_matrix <- data\n",
    "\n",
    "num_columns <- ncol(sparse_matrix)\n",
    "# Get the nonzero elements and their indices\n",
    "nonzero_elements <- sparse_matrix@x\n",
    "row_indices <- sparse_matrix@i\n",
    "col_indices <- rep(1:num_columns, times = diff(sparse_matrix@p))  # Use sparse_matrix@j for column indices\n",
    "\n",
    "\n",
    "nonzero_data <- data.frame(row_indices, col_indices, nonzero_elements)\n",
    "\n",
    "col_names <- data.frame(sparse_matrix@Dimnames[2])\n",
    "nrow(col_names)\n",
    "row_names <- data.frame((sparse_matrix@Dimnames[1]))\n",
    "\n",
    "write.csv(row_names, 'row_names.csv')\n",
    "write.csv(col_names, 'col_names.csv')\n",
    "\n",
    "\n",
    "# Save the i, j information to a Parquet file\n",
    "write_parquet(nonzero_data, \"non_zero.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv and paquet data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata csv file\n",
    "metadata_path = 'C:\\\\Users\\\\nphda\\\\OneDrive\\\\Desktop\\\\Data Mining\\\\Jackson-and-Anh-Data-Mining-Assingment-4\\\\data\\\\GSE185948_metadata_RNA.csv'\n",
    "metadata = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning sparse_matrix, column_names, and row_names\n"
     ]
    }
   ],
   "source": [
    "# read rna file from paquet\n",
    "data_path = 'C:\\\\Users\\\\nphda\\\\OneDrive\\\\Desktop\\\\Data Mining\\\\Jackson-and-Anh-Data-Mining-Assingment-4\\\\data\\\\non_zero.parquet'\n",
    "row_info_path = 'C:\\\\Users\\\\nphda\\\\OneDrive\\\\Desktop\\\\Data Mining\\\\Jackson-and-Anh-Data-Mining-Assingment-4\\\\data\\\\row_names.csv'\n",
    "column_info_path = 'C:\\\\Users\\\\nphda\\\\OneDrive\\\\Desktop\\\\Data Mining\\\\Jackson-and-Anh-Data-Mining-Assingment-4\\\\data\\\\col_names.csv'\n",
    "sparse_matrix, row_names, column_names = load_data(data_path, row_info_path, column_info_path)\n",
    "row_indices = np.arange(sparse_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to csr for faster computation\n",
    "sparse_matrix = sparse_matrix.tocsr(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy case\n",
    "sparse_matrix = sparse_matrix[0:100, 0:50]\n",
    "row_indices = np.arange(sparse_matrix.shape[0])\n",
    "row_names = row_names[0:100]\n",
    "column_names = column_names[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pipeline = Pipeline([\n",
    "    ('splitter', SparseTrainTestSplit(test_size=0.2, random_state=42)),\n",
    "    # Add other steps in the pipeline as needed\n",
    "])\n",
    "\n",
    "clean_and_pca_pipeline = Pipeline([\n",
    "    ('cleaner', DataClean()),  # CleanData is performed first\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('pca', TruncatedSVD(n_components=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the full data\n",
    "pca_sparse_full = clean_and_pca_pipeline.fit(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nphda\\miniconda3\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "sparse_train, sparse_test, train_indices, test_indices = split_pipeline.fit_transform(sparse_matrix)\n",
    "# Fit the pipeline to training\n",
    "clean_and_pca_pipeline.fit(sparse_train)\n",
    "# Transform the training data\n",
    "pca_sparse_train = clean_and_pca_pipeline.transform(sparse_train)\n",
    "# Transform the test data\n",
    "pca_sparse_test = clean_and_pca_pipeline.transform(sparse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_full = reindex(pca_sparse_full, indices=train_indices,  columns= column_names, names=row_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 1), indices imply (1, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nphda\\OneDrive\\Desktop\\Data Mining\\Jackson-and-Anh-Data-Mining-Assingment-4\\Scripts\\progress\\pretty.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nphda/OneDrive/Desktop/Data%20Mining/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/progress/pretty.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mPC1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPC2\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nphda/OneDrive/Desktop/Data%20Mining/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/progress/pretty.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_pca_df_test \u001b[39m=\u001b[39m reindex(pca_sparse_train, indices \u001b[39m=\u001b[39;49mtrain_indices,  columns  \u001b[39m=\u001b[39;49m column_names, names \u001b[39m=\u001b[39;49m row_names)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nphda/OneDrive/Desktop/Data%20Mining/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/progress/pretty.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_pca_df_test \u001b[39m=\u001b[39m reindex(pca_sparse_test, indices \u001b[39m=\u001b[39mtest_indices,  columns  \u001b[39m=\u001b[39m column_names, names \u001b[39m=\u001b[39m row_names)\n",
      "\u001b[1;32mc:\\Users\\nphda\\OneDrive\\Desktop\\Data Mining\\Jackson-and-Anh-Data-Mining-Assingment-4\\Scripts\\progress\\pretty.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nphda/OneDrive/Desktop/Data%20Mining/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/progress/pretty.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreindex\u001b[39m(df, indices, columns, names):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nphda/OneDrive/Desktop/Data%20Mining/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/progress/pretty.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     row_names_indices \u001b[39m=\u001b[39m [names[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nphda/OneDrive/Desktop/Data%20Mining/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/progress/pretty.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df_with_indices \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(df, columns\u001b[39m=\u001b[39;49mcolumns, index\u001b[39m=\u001b[39;49mrow_names_indices)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nphda/OneDrive/Desktop/Data%20Mining/Jackson-and-Anh-Data-Mining-Assingment-4/Scripts/progress/pretty.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df_with_indices\n",
      "File \u001b[1;32mc:\\Users\\nphda\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py:785\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    774\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    775\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    776\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    782\u001b[0m             copy\u001b[39m=\u001b[39m_copy,\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 785\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    786\u001b[0m             data,\n\u001b[0;32m    787\u001b[0m             index,\n\u001b[0;32m    788\u001b[0m             columns,\n\u001b[0;32m    789\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    790\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    791\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    794\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\nphda\\miniconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\nphda\\miniconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (1, 1), indices imply (1, 5)"
     ]
    }
   ],
   "source": [
    "columns=[\"PC1\", \"PC2\"]\n",
    "train_pca_df_test = reindex(pca_sparse_train, indices =train_indices,  columns  = column_names, names = row_names)\n",
    "test_pca_df_test = reindex(pca_sparse_test, indices =test_indices,  columns  = column_names, names = row_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_full.to_csv('pca_df_full', index=False)\n",
    "train_pca_df_test.to_csv('pca_train_df', index=False)\n",
    "test_pca_df_test.to_csv('pca_test_df', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_params = {\n",
    "    'n_clusters': list(range(1, 10)),\n",
    "    'init': ['random', 'k-means++'],\n",
    "    'n_init': [1, 5, 10],\n",
    "    'max_iter': [300],\n",
    "    'random_state': [0]\n",
    "}\n",
    "\n",
    "choice, k_means_estimator = optimize_and_compare_kmeans(train_without_outliers, kmeans_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_outliers_in_clusters(train_without_outliers, k_means_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_plot(train_without_outliers, k_means_estimator.labels_, \"K Means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for HDBSCAN\n",
    "hdbscan_params = {\n",
    "    'min_samples': [10, 30, 50, 60, 100],\n",
    "    'min_cluster_size': [100, 200, 300, 400, 500, 600],\n",
    "    'cluster_selection_method': ['eom', 'leaf'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Usage example with parameters\n",
    "result = optimize_and_compare_hdbscan(train_without_outliers, hdbscan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_plot(train_without_outliers, result.labels_, \"HDBSCAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Output/non_zero.parquet'\n",
    "row_info_path = '../Output/row_names.csv'\n",
    "column_info_path = '../Output/col_names.csv'\n",
    "#sparse_matrix = coo_matrix((data['nonzero_elements'], (data['row_indices'], data['col_indices'])))\n",
    "sparse_matrix_trans, row_names_trans, col_names_trans, row_indices_trans= my_utils.load_data(data_path, row_info_path, column_info_path, transpose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, columns, row_names):\n",
    "    # Preprocess the full data\n",
    "    full = clean_and_pca_pipeline.transform(df)\n",
    "    # Split data\n",
    "    train, test, train_indices, test_indices = split_pipeline.fit_transform(df)\n",
    "    # Fit the pipeline to full\n",
    "    clean_and_pca_pipeline.fit(full)\n",
    "    pca_full = clean_and_pca_pipeline.transform(full)\n",
    "    # Fit the pipeline to training\n",
    "    clean_and_pca_pipeline.fit(train)\n",
    "    # Transform the training data\n",
    "    pca_train = clean_and_pca_pipeline.transform(train)\n",
    "    # Transform the test data\n",
    "    pca_test = clean_and_pca_pipeline.transform(test)\n",
    "    # Reindex \n",
    "    columns=[\"PC1\", \"PC2\"]\n",
    "    pca_df_full = reindex(pca_full, indices=train_indices,  columns= columns, names=row_names)\n",
    "    pca_df_train = reindex(pca_train, indices =train_indices,  columns  = columns, names = row_names)\n",
    "    pca_df_test = reindex(pca_test, indices =test_indices,  columns  = columns, names = row_names)\n",
    "    # write to csv\n",
    "    pca_df_full.to_csv('pca_df_full', index=False)\n",
    "    train_pca_df_test.to_csv('pca_train_df', index=False)\n",
    "    test_pca_df_test.to_csv('pca_test_df', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sparse_matrix_trans, col_names_trans, row_names_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice, k_means_estimator = optimize_and_compare_kmeans(train_without_outliers, kmeans_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_plot(train_without_outliers, k_means_estimator.labels_, \"K Means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for HDBSCAN\n",
    "hdbscan_params = {\n",
    "    'min_samples': [10, 30, 50, 60, 100],\n",
    "    'min_cluster_size': [100, 200, 300, 400, 500, 600],\n",
    "    'cluster_selection_method': ['eom', 'leaf'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Usage example with parameters\n",
    "result = optimize_and_compare_hdbscan(train_without_outliers, hdbscan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_plot(train_without_outliers, result.labels_, \"HDBSCAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.merge(df, metadata, left_on='index', right_on='name', how='left').drop('name', axis=1)\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_demo = meta_df[['cluster', 'patient', 'gender', 'disease', 'celltype']]\n",
    "meta_df_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_demo.describe(include=['object']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_demo.groupby('cluster').describe(include=['object']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "brc = Birch(n_clusters=2)\n",
    "brc.fit(df_t)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(df_t, brc.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run time analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
